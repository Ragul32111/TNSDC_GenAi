{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMh3e6wUXAirE18+wsSyseg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"8M-xDzd48DHs"},"outputs":[],"source":[]},{"cell_type":"code","source":["import numpy as np\n","from tensorflow.keras.utils import get_file\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Embedding\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import os\n","# Load your dataset\n","# For example, you can use the Gutenberg corpus from NLTK\n","import nltk\n","nltk.download('gutenberg')\n","from nltk.corpus import gutenberg\n","text = gutenberg.raw('shakespeare-hamlet.txt')\n","\n","\n","# Preprocessing the text\n","tokenizer = tf.keras.preprocessing.text.Tokenizer()\n","tokenizer.fit_on_texts([text])\n","total_words = len(tokenizer.word_index) + 1\n","\n","# Creating input sequences and corresponding outputs\n","input_sequences = []\n","for line in text.split('\\n'):\n","    token_list = tokenizer.texts_to_sequences([line])[0]\n","    for i in range(1, len(token_list)):\n","        n_gram_sequence = token_list[:i+1]\n","        input_sequences.append(n_gram_sequence)\n","\n","# Padding sequences\n","max_sequence_len = max([len(x) for x in input_sequences])\n","input_sequences = np.array(tf.keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n","\n","# Creating predictors and label\n","X, y = input_sequences[:,:-1],input_sequences[:,-1]\n","y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n","\n","# Building the model\n","model = Sequential()\n","model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n","model.add(LSTM(150, return_sequences=True))\n","model.add(LSTM(100))\n","model.add(Dense(total_words, activation='softmax'))\n","\n","# Compiling the model\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.summary()\n","\n","# Training the model\n","model.fit(X, y, epochs=5, verbose=1)\n","def generate_story(seed_text, sentences, model, max_sequence_len):\n","    generated_story = seed_text\n","    for _ in range(sentences):\n","        for _ in range(10):  # Generate 10 words per sentence\n","            token_list = tokenizer.texts_to_sequences([seed_text])[0]\n","            token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n","            predicted_probs = model.predict(token_list, verbose=0)[0]\n","            predicted_index = np.random.choice(len(predicted_probs), p=predicted_probs)\n","            output_word = tokenizer.index_word[predicted_index] if predicted_index > 0 else ''\n","            seed_text += \" \" + output_word\n","            generated_story += \" \" + output_word\n","            if output_word == '.':\n","                break  # End the sentence if period (.) is generated\n","        seed_text = '.'  # Start a new sentence\n","        generated_story += \"\\n\"  # Start a new line for each new sentence\n","    return generated_story\n","\n","# Generate a story\n","generated_story = generate_story(\"to be or not to be\", 5, model, max_sequence_len)\n","print(generated_story)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e602e61e-56e7-461a-da9b-6b1b09da4c26","id":"nttF3hTZ0udz","executionInfo":{"status":"ok","timestamp":1712247168910,"user_tz":420,"elapsed":334686,"user":{"displayName":"nandha nandha","userId":"05018310402603508469"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]   Package gutenberg is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_2 (Embedding)     (None, 13, 100)           481800    \n","                                                                 \n"," lstm_4 (LSTM)               (None, 13, 150)           150600    \n","                                                                 \n"," lstm_5 (LSTM)               (None, 100)               100400    \n","                                                                 \n"," dense_2 (Dense)             (None, 4818)              486618    \n","                                                                 \n","=================================================================\n","Total params: 1219418 (4.65 MB)\n","Trainable params: 1219418 (4.65 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Epoch 1/5\n","805/805 [==============================] - 61s 71ms/step - loss: 6.8729 - accuracy: 0.0326\n","Epoch 2/5\n","805/805 [==============================] - 56s 70ms/step - loss: 6.4685 - accuracy: 0.0397\n","Epoch 3/5\n","805/805 [==============================] - 58s 72ms/step - loss: 6.2962 - accuracy: 0.0488\n","Epoch 4/5\n","805/805 [==============================] - 57s 71ms/step - loss: 6.1344 - accuracy: 0.0559\n","Epoch 5/5\n","805/805 [==============================] - 55s 69ms/step - loss: 5.9707 - accuracy: 0.0658\n","to be or not to be extent i in other strange of done is his vnction\n"," the vpspring patience for oh mad not along out the\n"," him i as liue in it shall i lord not\n"," mee infinite make that he hard offendendo hamlet it may\n"," the hammers depends of his farre ought of my greene\n","\n"]}]}]}